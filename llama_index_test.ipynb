{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Llaama index test notebook**\n",
    "Created by Sean Kan, 5 Jan 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-index in /Users/seankan/Library/Python/3.9/lib/python/site-packages (0.9.8.post1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.41.3.post2-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: aiostream<0.6.0,>=0.5.2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (0.5.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (4.12.2)\n",
      "Requirement already satisfied: dataclasses-json in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (0.6.3)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (1.2.14)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (2023.10.0)\n",
      "Requirement already satisfied: httpx in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (0.25.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (1.5.8)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (1.26.2)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (1.3.5)\n",
      "Requirement already satisfied: pandas in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (4.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (0.9.0)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.20.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from transformers) (23.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from transformers) (2023.10.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: psutil in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from accelerate) (5.9.6)\n",
      "Collecting torch>=1.10.0 (from accelerate)\n",
      "  Downloading torch-2.1.2-cp39-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (4.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index) (2.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from deprecated>=1.2.9.3->llama-index) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from openai>=1.1.0->llama-index) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from openai>=1.1.0->llama-index) (1.8.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from openai>=1.1.0->llama-index) (2.5.2)\n",
      "Requirement already satisfied: certifi in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from httpx->llama-index) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from httpx->llama-index) (1.0.2)\n",
      "Requirement already satisfied: idna in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from httpx->llama-index) (3.6)\n",
      "Requirement already satisfied: sniffio in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from httpx->llama-index) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx->llama-index) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from requests>=2.31.0->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from requests>=2.31.0->llama-index) (2.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.1)\n",
      "Collecting sympy (from torch>=1.10.0->accelerate)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx (from torch>=1.10.0->accelerate)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from typing-inspect>=0.8.0->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from dataclasses-json->llama-index) (3.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from pandas->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from pandas->llama-index) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from pandas->llama-index) (2023.3)\n",
      "Requirement already satisfied: exceptiongroup in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from anyio<4,>=3.5.0->openai>=1.1.0->llama-index) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index) (2.14.5)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->llama-index) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.10.0->accelerate)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.41.3.post2-py3-none-any.whl (92.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.20.1-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp39-cp39-macosx_11_0_arm64.whl (174 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.4/174.4 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp39-cp39-macosx_11_0_arm64.whl (426 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.1/426.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp39-cp39-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.2-cp39-none-macosx_11_0_arm64.whl (59.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, bitsandbytes, sympy, safetensors, pyyaml, networkx, filelock, torch, huggingface-hub, tokenizers, accelerate, transformers\n",
      "Successfully installed accelerate-0.25.0 bitsandbytes-0.41.3.post2 filelock-3.13.1 huggingface-hub-0.20.1 mpmath-1.3.0 networkx-3.2.1 pyyaml-6.0.1 safetensors-0.4.1 sympy-1.12 tokenizers-0.15.0 torch-2.1.2 transformers-4.36.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Level Concepts\n",
    "This is a quick guide to the high-level concepts you’ll encounter frequently when building LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Augmented Generation (RAG)\n",
    "LLMs are trained on enormous bodies of data but they aren’t trained on your data. Retrieval-Augmented Generation (RAG) solves this problem by adding your data to the data LLMs already have access to. You will see references to RAG frequently in this documentation.\n",
    "\n",
    "In RAG, your data is loaded and prepared for queries or “indexed”. User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.\n",
    "\n",
    "Even if what you’re building is a chatbot or an agent, you’ll want to know RAG techniques for getting data into your application.\n",
    "<img src=\"img/basic_rag.png\" alt=\"rag\" width=\"1000\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stages within RAG**\n",
    "There are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\n",
    "\n",
    "**Loading**: this refers to getting your data from where it lives – whether it’s text files, PDFs, another website, a database, or an API – into your pipeline. LlamaHub provides hundreds of connectors to choose from.\n",
    "\n",
    "**Indexing**: this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n",
    "\n",
    "**Storing**: once your data is indexed you will almost always want to store your index, as well as other metadata, to avoid having to re-index it.\n",
    "\n",
    "**Querying**: for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n",
    "\n",
    "**Evaluation**: a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rag_stage.png\" alt=\"alt text\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Important concepts within each step**\n",
    "There are also some terms you’ll encounter that refer to steps within each of these stages.\n",
    "\n",
    "#### **Loading stage**\n",
    "**Nodes and Documents**: A Document is a container around any data source - for instance, a PDF, an API output, or retrieve data from a database. A Node is the atomic unit of data in LlamaIndex and represents a “chunk” of a source Document. Nodes have metadata that relate them to the document they are in and to other nodes.\n",
    "\n",
    "**Connectors**: A data connector (often called a Reader) ingests data from different data sources and data formats into Documents and Nodes.\n",
    "\n",
    "#### **Indexing Stage**\n",
    "**Indexes**: Once you’ve ingested your data, LlamaIndex will help you index the data into a structure that’s easy to retrieve. This usually involves generating vector embeddings which are stored in a specialized database called a vector store. Indexes can also store a variety of metadata about your data.\n",
    "\n",
    "**Embeddings** LLMs generate numerical representations of data called embeddings. When filtering your data for relevance, LlamaIndex will convert queries into embeddings, and your vector store will find data that is numerically similar to the embedding of your query.\n",
    "\n",
    "#### **Querying Stage**\n",
    "**Retrievers**: A retriever defines how to efficiently retrieve relevant context from an index when given a query. Your retrieval strategy is key to the relevancy of the data retrieved and the efficiency with which it’s done.\n",
    "\n",
    "**Routers**: A router determines which retriever will be used to retrieve relevant context from the knowledge base. More specifically, the RouterRetriever class, is responsible for selecting one or multiple candidate retrievers to execute a query. They use a selector to choose the best option based on each candidate’s metadata and the query.\n",
    "\n",
    "**Node Postprocessors**: A node postprocessor takes in a set of retrieved nodes and applies transformations, filtering, or re-ranking logic to them.\n",
    "\n",
    "**Response Synthesizers**: A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\n",
    "\n",
    "#### **Putting it all together**\n",
    "There are endless use cases for data-backed LLM applications but they can be roughly grouped into three categories:\n",
    "\n",
    "**Query Engines**: A query engine is an end-to-end pipeline that allows you to ask questions over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.\n",
    "\n",
    "**Chat Engines**: A chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question-and-answer).\n",
    "\n",
    "**Agents**: An agent is an automated decision-maker powered by an LLM that interacts with the world via a set of tools. Agents can take an arbitrary number of steps to complete a given task, dynamically deciding on the best course of action rather than following pre-determined steps. This gives it additional flexibility to tackle more complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration\n",
    "#### Data preparation\n",
    "Only two types of data will be demonstrated in this notebook: unstructed documents (txt, pdf) and API (wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-05 15:58:00--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham/paul_graham_essay1.txt’\n",
      "\n",
      "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2024-01-05 15:58:01 (1.31 MB/s) - ‘data/paul_graham/paul_graham_essay1.txt’ saved [75042/75042]\n",
      "\n",
      "--2024-01-05 15:58:01--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham/paul_graham_essay2.txt’\n",
      "\n",
      "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2024-01-05 15:58:01 (1.13 MB/s) - ‘data/paul_graham/paul_graham_essay2.txt’ saved [75042/75042]\n",
      "\n",
      "--2024-01-05 15:58:01--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham/paul_graham_essay3.txt’\n",
      "\n",
      "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2024-01-05 15:58:02 (1.15 MB/s) - ‘data/paul_graham/paul_graham_essay3.txt’ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download llama_index sample data, paul graham essays\n",
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay1.txt'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay2.txt'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay3.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the LLM\n",
    "For some reason, indexing services requires OpenAI key and therefore we will prepare a local customised llm before indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 684/684 [00:00<00:00, 713kB/s]\n",
      "model.safetensors: 100%|██████████| 133M/133M [00:05<00:00, 23.1MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 366/366 [00:00<00:00, 149kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 5.44MB/s]\n",
      "tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 1.73MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 57.7kB/s]\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/seankan/Library/Caches/llama_index...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import Ollama\n",
    "llm = Ollama(model=\"mixtral\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X44sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m   prompt \u001b[39m=\u001b[39m prompt \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<|assistant|>\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X44sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m prompt\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X44sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m llm \u001b[39m=\u001b[39m HuggingFaceLLM(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X44sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHuggingFaceH4/zephyr-7b-beta\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X44sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     tokenizer_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHuggingFaceH4/zephyr-7b-beta\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X44sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     query_wrapper_prompt\u001b[39m=\u001b[39;49mPromptTemplate(\u001b[39m\"\u001b[39;49m\u001b[39m<|system|>\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m</s>\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m<|user|>\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m{query_str}\u001b[39;49;00m\u001b[39m</s>\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m<|assistant|>\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X44sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     context_window\u001b[39m=\u001b[39;49m\u001b[39m3900\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X44sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X44sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     model_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mquantization_config\u001b[39;49m\u001b[39m\"\u001b[39;49m: quantization_config},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X44sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# tokenizer_kwargs={},\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X44sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     generate_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0.7\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtop_k\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m50\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0.95\u001b[39;49m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X44sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     messages_to_prompt\u001b[39m=\u001b[39;49mmessages_to_prompt,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X44sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39m# Changed to mps for apple silicon chips\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X44sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmps\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X44sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/llms/huggingface.py:160\u001b[0m, in \u001b[0;36mHuggingFaceLLM.__init__\u001b[0;34m(self, context_window, max_new_tokens, system_prompt, query_wrapper_prompt, tokenizer_name, model_name, model, tokenizer, device_map, stopping_ids, tokenizer_kwargs, tokenizer_outputs_to_remove, model_kwargs, generate_kwargs, messages_to_prompt, callback_manager)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m requires torch and transformers packages.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install both with `pip install transformers[torch]`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[1;32m    159\u001b[0m model_kwargs \u001b[39m=\u001b[39m model_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m--> 160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    161\u001b[0m     model_name, device_map\u001b[39m=\u001b[39;49mdevice_map, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs\n\u001b[1;32m    162\u001b[0m )\n\u001b[1;32m    164\u001b[0m \u001b[39m# check context_window\u001b[39;00m\n\u001b[1;32m    165\u001b[0m config_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mto_dict()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:2897\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m \u001b[39mif\u001b[39;00m load_in_8bit \u001b[39mor\u001b[39;00m load_in_4bit:\n\u001b[1;32m   2896\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m-> 2897\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2898\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_accelerate_available() \u001b[39mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[1;32m   2899\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m   2900\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2901\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2902\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m `pip install bitsandbytes`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2903\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from llama_index.prompts import PromptTemplate\n",
    "# from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.llms import Ollama\n",
    "\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "  prompt = \"\"\n",
    "  for message in messages:\n",
    "    if message.role == 'system':\n",
    "      prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "    elif message.role == 'user':\n",
    "      prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "    elif message.role == 'assistant':\n",
    "      prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "\n",
    "  # ensure we start with a system prompt, insert blank if needed\n",
    "  if not prompt.startswith(\"<|system|>\\n\"):\n",
    "    prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
    "\n",
    "  # add final assistant prompt\n",
    "  prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "  return prompt\n",
    "\n",
    "\n",
    "# llm = HuggingFaceLLM(\n",
    "#     model_name=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "#     tokenizer_name=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "#     query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),\n",
    "#     context_window=3900,\n",
    "#     max_new_tokens=256,\n",
    "#     model_kwargs={\"quantization_config\": quantization_config},\n",
    "#     # tokenizer_kwargs={},\n",
    "#     generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "#     messages_to_prompt=messages_to_prompt,\n",
    "#     # Changed to mps for apple silicon chips\n",
    "#     device_map='mps',\n",
    "# )\n",
    "\n",
    "llm = Ollama(\n",
    "  model=\"mixtral\",\n",
    "  query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),\n",
    "    context_window=3900,\n",
    "    max_new_tokens=256,\n",
    "    model_kwargs={\"quantization_config\": quantization_config},\n",
    "    # tokenizer_kwargs={},\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    # Changed to mps for apple silicon chips\n",
    "    device_map='mps',\n",
    "  )\n",
    "service_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load the data into index**\n",
    "The simplest way to use a Vector Store is to load a set of documents and build an index from them using \n",
    "\n",
    "```python\n",
    "from_documents\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nbconvert\n",
      "  Downloading nbconvert-7.14.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (4.12.2)\n",
      "Collecting bleach!=5.0.0 (from nbconvert)\n",
      "  Downloading bleach-6.1.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting defusedxml (from nbconvert)\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (6.8.0)\n",
      "Collecting jinja2>=3.0 (from nbconvert)\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jupyter-core>=4.7 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (5.5.0)\n",
      "Collecting jupyterlab-pygments (from nbconvert)\n",
      "  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting markupsafe>=2.0 (from nbconvert)\n",
      "  Downloading MarkupSafe-2.1.3-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert)\n",
      "  Downloading mistune-3.0.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert)\n",
      "  Downloading nbclient-0.9.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting nbformat>=5.7 (from nbconvert)\n",
      "  Downloading nbformat-5.9.2-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: packaging in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (23.2)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert)\n",
      "  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (2.17.2)\n",
      "Collecting tinycss2 (from nbconvert)\n",
      "  Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: traitlets>=5.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (5.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from bleach!=5.0.0->nbconvert) (1.15.0)\n",
      "Collecting webencodings (from bleach!=5.0.0->nbconvert)\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=3.6->nbconvert) (3.17.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from jupyter-core>=4.7->nbconvert) (4.0.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbclient>=0.5.0->nbconvert) (8.6.0)\n",
      "Collecting fastjsonschema (from nbformat>=5.7->nbconvert)\n",
      "  Downloading fastjsonschema-2.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting jsonschema>=2.6 (from nbformat>=5.7->nbconvert)\n",
      "  Downloading jsonschema-4.20.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4->nbconvert) (2.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (23.1.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.6->nbformat>=5.7->nbconvert)\n",
      "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=2.6->nbformat>=5.7->nbconvert)\n",
      "  Downloading referencing-0.32.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=2.6->nbformat>=5.7->nbconvert)\n",
      "  Downloading rpds_py-0.16.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (25.1.1)\n",
      "Requirement already satisfied: tornado>=6.2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.3.3)\n",
      "Downloading nbconvert-7.14.0-py3-none-any.whl (256 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.4/256.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading bleach-6.1.0-py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.8/162.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.3-cp39-cp39-macosx_10_9_universal2.whl (17 kB)\n",
      "Downloading mistune-3.0.2-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nbclient-0.9.0-py3-none-any.whl (24 kB)\n",
      "Downloading nbformat-5.9.2-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.6/77.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading jsonschema-4.20.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastjsonschema-2.19.1-py3-none-any.whl (23 kB)\n",
      "Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Downloading referencing-0.32.0-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.16.2-cp39-cp39-macosx_11_0_arm64.whl (332 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.2/332.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: webencodings, fastjsonschema, tinycss2, rpds-py, pandocfilters, mistune, markupsafe, jupyterlab-pygments, defusedxml, bleach, referencing, jinja2, jsonschema-specifications, jsonschema, nbformat, nbclient, nbconvert\n",
      "Successfully installed bleach-6.1.0 defusedxml-0.7.1 fastjsonschema-2.19.1 jinja2-3.1.2 jsonschema-4.20.0 jsonschema-specifications-2023.12.1 jupyterlab-pygments-0.3.0 markupsafe-2.1.3 mistune-3.0.2 nbclient-0.9.0 nbconvert-7.14.0 nbformat-5.9.2 pandocfilters-1.5.0 referencing-0.32.0 rpds-py-0.16.2 tinycss2-1.2.1 webencodings-0.5.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NotImplementedType' object has no attribute 'callback_manager'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m \u001b[39mimport\u001b[39;00m ServiceContext\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m service_context \u001b[39m=\u001b[39m ServiceContext\u001b[39m.\u001b[39;49mfrom_defaults(llm\u001b[39m=\u001b[39;49m\u001b[39mNotImplemented\u001b[39;49m, embed_model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlocal:BAAI/bge-small-en-v1.5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/service_context.py:171\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[0;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, text_splitter, transformations, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[1;32m    167\u001b[0m llm_predictor \u001b[39m=\u001b[39m llm_predictor \u001b[39mor\u001b[39;00m LLMPredictor(\n\u001b[1;32m    168\u001b[0m     llm\u001b[39m=\u001b[39mllm, pydantic_program_mode\u001b[39m=\u001b[39mpydantic_program_mode\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(llm_predictor, LLMPredictor):\n\u001b[0;32m--> 171\u001b[0m     llm_predictor\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mcallback_manager \u001b[39m=\u001b[39m callback_manager\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;00m system_prompt:\n\u001b[1;32m    173\u001b[0m         llm_predictor\u001b[39m.\u001b[39msystem_prompt \u001b[39m=\u001b[39m system_prompt\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NotImplementedType' object has no attribute 'callback_manager'"
     ]
    }
   ],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=null, embed_model=\"local:BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No files found in data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m PERSIST_DIR \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./storage\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(PERSIST_DIR):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# load the documents and create the index\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     documents \u001b[39m=\u001b[39m SimpleDirectoryReader(\u001b[39m\"\u001b[39;49m\u001b[39m./data/.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mload_data()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X34sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     index \u001b[39m=\u001b[39m VectorStoreIndex\u001b[39m.\u001b[39mfrom_documents(documents)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seankan/Desktop/llama_index_test/llama_index_test.ipynb#X34sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# store it for later\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/readers/file/base.py:143\u001b[0m, in \u001b[0;36mSimpleDirectoryReader.__init__\u001b[0;34m(self, input_dir, input_files, exclude, exclude_hidden, errors, recursive, encoding, filename_as_id, required_exts, file_extractor, num_files_limit, file_metadata)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_dir \u001b[39m=\u001b[39m Path(input_dir)\n\u001b[1;32m    142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexclude \u001b[39m=\u001b[39m exclude\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_files \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_files(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_dir)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m file_extractor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_extractor \u001b[39m=\u001b[39m file_extractor\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/readers/file/base.py:204\u001b[0m, in \u001b[0;36mSimpleDirectoryReader._add_files\u001b[0;34m(self, input_dir)\u001b[0m\n\u001b[1;32m    201\u001b[0m new_input_files \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(all_files)\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(new_input_files) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo files found in \u001b[39m\u001b[39m{\u001b[39;00minput_dir\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    206\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_files_limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_files_limit \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    207\u001b[0m     new_input_files \u001b[39m=\u001b[39m new_input_files[\u001b[39m0\u001b[39m : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_files_limit]\n",
      "\u001b[0;31mValueError\u001b[0m: No files found in data."
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"./data/.\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
