{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Llaama Index Notebook**\n",
    "Created by Sean Kan, 5 Jan 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-index in /Users/seankan/Library/Python/3.9/lib/python/site-packages (0.9.8.post1)\n",
      "Requirement already satisfied: transformers in /Users/seankan/Library/Python/3.9/lib/python/site-packages (4.36.2)\n",
      "Requirement already satisfied: accelerate in /Users/seankan/Library/Python/3.9/lib/python/site-packages (0.25.0)\n",
      "Requirement already satisfied: bitsandbytes in /Users/seankan/Library/Python/3.9/lib/python/site-packages (0.41.3.post2)\n",
      "Requirement already satisfied: matplotlib in /Users/seankan/Library/Python/3.9/lib/python/site-packages (3.8.2)\n",
      "Requirement already satisfied: pypdf in /Users/seankan/Library/Python/3.9/lib/python/site-packages (3.17.4)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: aiostream<0.6.0,>=0.5.2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (0.5.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (4.12.2)\n",
      "Requirement already satisfied: dataclasses-json in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (0.6.3)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (1.2.14)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (2023.10.0)\n",
      "Requirement already satisfied: httpx in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (0.25.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (1.5.8)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (1.26.2)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (1.3.5)\n",
      "Requirement already satisfied: pandas in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (4.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from llama-index) (0.9.0)\n",
      "Requirement already satisfied: filelock in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: psutil in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (4.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index) (2.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from deprecated>=1.2.9.3->llama-index) (1.16.0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index transformers accelerate bitsandbytes matplotlib pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Level Concepts\n",
    "This is a quick guide to the high-level concepts you’ll encounter frequently when building LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Augmented Generation (RAG)\n",
    "LLMs are trained on enormous bodies of data but they aren’t trained on your data. Retrieval-Augmented Generation (RAG) solves this problem by adding your data to the data LLMs already have access to. You will see references to RAG frequently in this documentation.\n",
    "\n",
    "In RAG, your data is loaded and prepared for queries or “indexed”. User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.\n",
    "\n",
    "Even if what you’re building is a chatbot or an agent, you’ll want to know RAG techniques for getting data into your application.\n",
    "<img src=\"img/basic_rag.png\" alt=\"rag\" width=\"1000\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stages within RAG**\n",
    "There are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\n",
    "\n",
    "**Loading**: this refers to getting your data from where it lives – whether it’s text files, PDFs, another website, a database, or an API – into your pipeline. LlamaHub provides hundreds of connectors to choose from.\n",
    "\n",
    "**Indexing**: this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n",
    "\n",
    "**Storing**: once your data is indexed you will almost always want to store your index, as well as other metadata, to avoid having to re-index it.\n",
    "\n",
    "**Querying**: for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n",
    "\n",
    "**Evaluation**: a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are.\n",
    "\n",
    "\n",
    "<img src=\"img/rag_stage.png\" alt=\"alt text\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Important concepts within each step**\n",
    "There are also some terms you’ll encounter that refer to steps within each of these stages.\n",
    "\n",
    "#### **Loading stage**\n",
    "**Nodes and Documents**: A Document is a container around any data source - for instance, a PDF, an API output, or retrieve data from a database. A Node is the atomic unit of data in LlamaIndex and represents a “chunk” of a source Document. Nodes have metadata that relate them to the document they are in and to other nodes.\n",
    "\n",
    "**Connectors**: A data connector (often called a Reader) ingests data from different data sources and data formats into Documents and Nodes.\n",
    "\n",
    "#### **Indexing Stage**\n",
    "**Indexes**: Once you’ve ingested your data, LlamaIndex will help you index the data into a structure that’s easy to retrieve. This usually involves generating vector embeddings which are stored in a specialized database called a vector store. Indexes can also store a variety of metadata about your data.\n",
    "\n",
    "**Embeddings** LLMs generate numerical representations of data called embeddings. When filtering your data for relevance, LlamaIndex will convert queries into embeddings, and your vector store will find data that is numerically similar to the embedding of your query.\n",
    "\n",
    "#### **Querying Stage**\n",
    "**Retrievers**: A retriever defines how to efficiently retrieve relevant context from an index when given a query. Your retrieval strategy is key to the relevancy of the data retrieved and the efficiency with which it’s done.\n",
    "\n",
    "**Routers**: A router determines which retriever will be used to retrieve relevant context from the knowledge base. More specifically, the RouterRetriever class, is responsible for selecting one or multiple candidate retrievers to execute a query. They use a selector to choose the best option based on each candidate’s metadata and the query.\n",
    "\n",
    "**Node Postprocessors**: A node postprocessor takes in a set of retrieved nodes and applies transformations, filtering, or re-ranking logic to them.\n",
    "\n",
    "**Response Synthesizers**: A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\n",
    "\n",
    "#### **Putting it all together**\n",
    "There are endless use cases for data-backed LLM applications but they can be roughly grouped into three categories:\n",
    "\n",
    "**Query Engines**: A query engine is an end-to-end pipeline that allows you to ask questions over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.\n",
    "\n",
    "**Chat Engines**: A chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question-and-answer).\n",
    "\n",
    "**Agents**: An agent is an automated decision-maker powered by an LLM that interacts with the world via a set of tools. Agents can take an arbitrary number of steps to complete a given task, dynamically deciding on the best course of action rather than following pre-determined steps. This gives it additional flexibility to tackle more complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration\n",
    "#### Data preparation\n",
    "Only two types of data will be demonstrated in this notebook: unstructed documents (txt, pdf) and API (wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-05 15:58:00--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham/paul_graham_essay1.txt’\n",
      "\n",
      "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2024-01-05 15:58:01 (1.31 MB/s) - ‘data/paul_graham/paul_graham_essay1.txt’ saved [75042/75042]\n",
      "\n",
      "--2024-01-05 15:58:01--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham/paul_graham_essay2.txt’\n",
      "\n",
      "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2024-01-05 15:58:01 (1.13 MB/s) - ‘data/paul_graham/paul_graham_essay2.txt’ saved [75042/75042]\n",
      "\n",
      "--2024-01-05 15:58:01--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham/paul_graham_essay3.txt’\n",
      "\n",
      "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2024-01-05 15:58:02 (1.15 MB/s) - ‘data/paul_graham/paul_graham_essay3.txt’ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download llama_index sample data, paul graham essays\n",
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay1.txt'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay2.txt'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay3.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the LLM\n",
    "For some reason, indexing services requires OpenAI key and therefore we will prepare a local customised llm before indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seankan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# from llama_index.prompts import PromptTemplate\n",
    "# from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.llms import Ollama\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     # load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     )\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "  prompt = \"\"\n",
    "  for message in messages:\n",
    "    if message.role == 'system':\n",
    "      prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "    elif message.role == 'user':\n",
    "      prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "    elif message.role == 'assistant':\n",
    "      prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "\n",
    "  # ensure we start with a system prompt, insert blank if needed\n",
    "  if not prompt.startswith(\"<|system|>\\n\"):\n",
    "    prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
    "\n",
    "  # add final assistant prompt\n",
    "  prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "  return prompt\n",
    "\n",
    "\n",
    "# llm = HuggingFaceLLM(\n",
    "#     model_name=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "#     tokenizer_name=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "#     query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),\n",
    "#     context_window=3900,\n",
    "#     max_new_tokens=256,\n",
    "#     model_kwargs={\"quantization_config\": quantization_config},\n",
    "#     # tokenizer_kwargs={},\n",
    "#     generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "#     messages_to_prompt=messages_to_prompt,\n",
    "#     # # Changed to mps for apple silicon chips\n",
    "#     # device_map='mps',\n",
    "#     # Normal GPU run\n",
    "#     device_map='auto',\n",
    "# )\n",
    "\n",
    "# # llama2 model\n",
    "# llm = Ollama(\n",
    "#   model=\"llama2\",\n",
    "#   context_window=3900,\n",
    "#   messages_to_prompt=messages_to_prompt,\n",
    "#   temperature = 0.7,\n",
    "#   additional_kwargs={\"top_k\": 50, \"top_p\": 0.95}\n",
    "#   )\n",
    "\n",
    "# # zephyr model\n",
    "# llm = Ollama(\n",
    "#   model=\"zephyr\",\n",
    "#   context_window=3900,\n",
    "#   messages_to_prompt=messages_to_prompt,\n",
    "#   temperature = 0.7,\n",
    "#   additional_kwargs={\"top_k\": 50, \"top_p\": 0.95}\n",
    "#   )\n",
    "\n",
    "# Mixtral 8*7B model\n",
    "llm = Ollama(\n",
    "  model=\"mixtral\",\n",
    "  context_window=3900,\n",
    "  messages_to_prompt=messages_to_prompt,\n",
    "  temperature = 0.7,\n",
    "  additional_kwargs={\"top_k\": 50, \"top_p\": 0.95}\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Indexing**\n",
    "With your data loaded, you now have a list of Document objects (or a list of Nodes). It’s time to build an Index over these objects so you can start querying them.\n",
    "\n",
    "#### **What is an Index?**\n",
    "In LlamaIndex terms, an Index is a data structure composed of Document objects, designed to enable querying by an LLM. Your Index is designed to be complementary to your querying strategy.\n",
    "\n",
    "LlamaIndex offers several different index types. We’ll cover the two most common here.\n",
    "\n",
    "#### **Vector Store Index**\n",
    "A VectorStoreIndex is by far the most frequent type of Index you’ll encounter. The Vector Store Index takes your Documents and splits them up into Nodes. It then creates vector embeddings of the text of every node, ready to be queried by an LLM.\n",
    "\n",
    "#### **What is an embedding?**\n",
    "Vector embeddings are central to how LLM applications function.\n",
    "\n",
    "A vector embedding, often just called an embedding, is a numerical representation of the semantics, or meaning of your text. Two pieces of text with similar meanings will have mathematically similar embeddings, even if the actual text is quite different.\n",
    "\n",
    "This mathematical relationship enables semantic search, where a user provides query terms and LlamaIndex can locate text that is related to the meaning of the query terms rather than simple keyword matching. This is a big part of how Retrieval-Augmented Generation works, and how LLMs function in general.\n",
    "\n",
    "There are many types of embeddings, and they vary in efficiency, effectiveness and computational cost. By default LlamaIndex uses text-embedding-ada-002, which is the default embedding used by OpenAI. If you are using different LLMs you will often want to use different embeddings.\n",
    "\n",
    "#### **Vector Store Index embeds your documents**\n",
    "Vector Store Index turns all of your text into embeddings using an API from your LLM; this is what is meant when we say it “embeds your text”. If you have a lot of text, generating embeddings can take a long time since it involves many round-trip API calls.\n",
    "\n",
    "When you want to search your embeddings, your query is itself turned into a vector embedding, and then a mathematical operation is carried out by VectorStoreIndex to rank all the embeddings by how semantically similar they are to your query.\n",
    "\n",
    "#### **Top K Retrieval**\n",
    "Once the ranking is complete, VectorStoreIndex returns the most-similar embeddings as their corresponding chunks of text. The number of embeddings it returns is known as k, so the parameter controlling how many embeddings to return is known as top_k. This whole type of search is often referred to as “top-k semantic retrieval” for this reason.\n",
    "\n",
    "Top-k retrieval is the simplest form of querying a vector index; you will learn about more complex and subtler strategies when you read the querying section.\n",
    "\n",
    "### **Load the data into index**\n",
    "The simplest way to use a Vector Store is to load a set of documents and build an index from them using \n",
    "\n",
    "\n",
    "```python\n",
    "from_documents\n",
    "```\n",
    "### **Creating LlamaIndex Index**\n",
    "The core essence of LlamaIndex lies in its ability to build structured indices over ingested data, represented as either Documents or Nodes. This indexing facilitates efficient querying over the data. Let's delve into how to build indices with both Document and Node objects, and what happens under the hood during this process.\n",
    "\n",
    "Different types of indices in LlamaIndex handle data in distinct ways:\n",
    "\n",
    "1. **Summary Index**: Stores Nodes as a sequential chain, and during query time, all Nodes are loaded into the Response Synthesis module if no other query parameters are specified.\n",
    "\n",
    "1. **Vector Store Index**: Stores each Node and a corresponding embedding in a Vector Store, and queries involve fetching the top-k most similar Nodes.\n",
    "\n",
    "1.  **Tree Index**: Builds a hierarchical tree from a set of Nodes, and queries involve traversing from root nodes down to leaf nodes.\n",
    "\n",
    "1. **Keyword Table Index**: Extracts keywords from each Node to build a mapping, and queries extract relevant keywords to fetch corresponding Nodes.\n",
    "\n",
    "To choose your index, you should carefully evaluate the module guides here and make a choice here according to your use case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nbconvert in /Users/seankan/Library/Python/3.9/lib/python/site-packages (7.14.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (6.8.0)\n",
      "Requirement already satisfied: jinja2>=3.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (3.1.2)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (5.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (2.1.3)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (0.9.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (5.9.2)\n",
      "Requirement already satisfied: packaging in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (23.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (1.5.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (2.17.2)\n",
      "Requirement already satisfied: tinycss2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (1.2.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbconvert) (5.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from bleach!=5.0.0->nbconvert) (1.15.0)\n",
      "Requirement already satisfied: webencodings in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from bleach!=5.0.0->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=3.6->nbconvert) (3.17.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from jupyter-core>=4.7->nbconvert) (4.0.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbclient>=0.5.0->nbconvert) (8.6.0)\n",
      "Requirement already satisfied: fastjsonschema in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbformat>=5.7->nbconvert) (2.19.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from nbformat>=5.7->nbconvert) (4.20.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4->nbconvert) (2.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.32.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.16.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (25.1.1)\n",
      "Requirement already satisfied: tornado>=6.2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.3.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__CUDNN VERSION: 8902\n",
      "__Number CUDA Devices: 1\n",
      "__CUDA Device Name: NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "__CUDA Device Total Memory [GB]: 8.589410304\n",
      "__CUDA Max Memory Allocated [GB]: 0.133448192\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# use_cuda = torch.cuda.is_available()\n",
    "# if use_cuda:\n",
    "#     print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "#     print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "#     print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "#     print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)\n",
    "#     print('__CUDA Max Memory Allocated [GB]:', torch.cuda.max_memory_allocated()/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index.response.notebook_utils import display_response\n",
    "\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data/paul_graham/\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context,service_context=service_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine(response_mode = \"compact\")\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The author's hobbies outside of school before college were writing short stories and programming on an IBM 1401 in their junior high school's basement. However, due to limitations of the machine without input or storage capabilities, the author couldn't remember any programs they wrote for the IBM 1401 as it had limited options without data stored on punch cards. The author was more impressed watching a friend build his own computer with a Heathkit kit and later convinced their father to buy them a TRS-80 microcomputer, which they used to program simple games, predict how high their model rockets would fly, and create a word processor for their father to write books. The author didn't initially plan to study programming in college but switched from philosophy to AI after finding the latter subject more interesting due to its practical applications shown in novels and documentaries like Heinlein's \"The Moon is a Harsh Mistress\" and a PBS documentary featuring Terry Winograd using SHRDLU."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(response_mode = \"refine\")\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Demonstration of API doc**\n",
    "URL and Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers import BeautifulSoupWebReader\n",
    "\n",
    "url = \"https://www.scmp.com/business/china-business/article/3247567/chinese-ev-maker-geely-introduces-first-pure-electric-galaxy-model-woo-mainstream-buyers-byd-foreign\"\n",
    "\n",
    "documents = BeautifulSoupWebReader().load_data([url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SummaryIndex\n",
    "\n",
    "index = SummaryIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Geely, a major Chinese private carmaker, has released its first fully electric sedan under the Galaxy brand for mass-market buyers.\n",
      "- The Galaxy E8 starts at 175,800 yuan (approximately US$24,752) and boasts a driving range of 550 km, making it cheaper and more efficient than BYD's Han model.\n",
      "- Geely aims to offer seven models under the Galaxy brand by 2025 for budget-conscious consumers, with a focus on safety, design, performance, and intelligence.\n",
      "- The E8 uses cutting-edge technology from companies like Qualcomm and BOE Technology, including a 45-inch screen and a Snapdragon 8295 chip for intelligent features.\n",
      "- Geely's parent company, Zhejiang Geely Holding Group, also owns other well-known marques such as Volvo, Lotus, and Lynk.\n",
      "- The release of the Galaxy E8 comes amid intensifying competition in China's electric car market, with rival companies like BYD and Tesla competing for dominance.\n",
      "- Chinese EV sales are expected to grow by 20% in 2024, according to a Fitch Ratings report, although only a few makers, such as BYD and Li Auto, are currently profitable.\n",
      "- Geely has formed a partnership with Shanghai-based Nio to promote battery swapping technology and improve charging infrastructure for electric cars.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(response_mode = \"compact\")\n",
    "response = query_engine.query(\"summarise the context in bullet points\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Geely Automobile Group, a major private carmaker in China, has introduced its first pure electric sedan under the Galaxy brand for mainstream buyers.\n",
      "- The new model, called E8, costs nearly $25,000 and has a driving range of 550km.\n",
      "- This is less expensive than BYD's Han EV, which sells for almost $30,000 and has a driving range of 506km.\n",
      "- The E8 is more affordable than Geely's Zeekr-branded electric cars, which target affluent buyers and compete with premium models made by companies such as Tesla.\n",
      "- The company aims to offer seven Galaxy branded models by 2025 and has a nearly 6% share of China's EV market.\n",
      "- Battery-swapping technology is being promoted by Geely's parent company and Shanghai-based Nio to address the lack of charging infrastructure for electric cars in China.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(response_mode = \"refine\")\n",
    "response = query_engine.query(\"summarise the context in bullet points\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Demo of reading Chinese website**\n",
    "When using llama_index to load and search a Chinese-written document, the indexing process involves converting the Chinese text into a format that Zephyr (or any other English-language LLM) can understand. This is \n",
    "typically done through optical character recognition (OCR), a technique that converts scanned or digital images of text into machine-readable text.\n",
    "\n",
    "The OCR process extracts text from images and converts Chinese characters into their corresponding Unicode codes, which are then passed to the indexing algorithm as searchable text. This allows users to search for \n",
    "specific keywords or phrases in the Chinese document using an English-language LLM like Zephyr, even though it cannot directly read or understand Chinese.\n",
    "\n",
    "During the search process, llama_index compares the user's query against the OCR-generated text, returning relevant results based on the similarity between the query and the indexed text. While this method is effective \n",
    "for simple searches, it may not be as accurate as using a Chinese LLM to search Chinese text due to differences in language structure and semantics. However, it can still provide useful information for those who do not \n",
    "have access to a Chinese-language LLM or prefer to use an English-language interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers import BeautifulSoupWebReader\n",
    "\n",
    "url = \"https://news.mingpao.com/pns/要聞/article/20240108/s00001/1704651224542/15立會議員去年質詢不足5次-「零蛋」蘇長荣-值得問的已藉其他方式表達\"\n",
    "\n",
    "documents = BeautifulSoupWebReader().load_data([url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SummaryIndex\n",
    "\n",
    "index = SummaryIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** In summary, 15 out of 120 legislators in the previous year did not raise enough questions during official parliamentary sessions, with Legislator Su Chao-cheng from the opposition party being the one who made the fewest number of inquiries. Some lawmakers have communicated their concerns through other methods instead of asking questions during these sessions, resulting in low numbers of questions raised. This reflects poorly on legislators' oversight functions, and it is suggested that both the quantity and quality of queries should be assessed to avoid unnecessary duplication. The article also briefly mentions public service fees for users as a possible area for further scrutiny. Some issues received multiple inquiries from lawmakers, including development and works projects initiated by New People's Party Chairperson Rachel Ngai Yuen-kwan and the medical policies being pursued by Legislator Gilbert Chen Ka-fai. However, political analyst Liu Tak-shia cautioned that low numbers of questions reflect poorly on legislators' oversight functions, and it is recommended that lawmakers demonstrate genuine engagement with policy issues being debated in parliament to avoid superficial or repetitive queries that waste both parliamentary time and resources."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(response_mode = \"refine\")\n",
    "response = query_engine.query(\"summarise the context in bullet points\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sub Question Query Engine Demo**\n",
    "Use a sub question query engine to tackle the problem of answering a complex query using multiple data sources.\n",
    "It first breaks down the complex query into sub questions for each relevant data source, then gather all the intermediate reponses and synthesizes a final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting wikipedia~=1.4 (from -r /Users/seankan/Library/Python/3.9/lib/python/site-packages/llama_index/download/llamahub_modules/requirements.txt (line 1))\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from wikipedia~=1.4->-r /Users/seankan/Library/Python/3.9/lib/python/site-packages/llama_index/download/llamahub_modules/requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from wikipedia~=1.4->-r /Users/seankan/Library/Python/3.9/lib/python/site-packages/llama_index/download/llamahub_modules/requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.0.0->wikipedia~=1.4->-r /Users/seankan/Library/Python/3.9/lib/python/site-packages/llama_index/download/llamahub_modules/requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.0.0->wikipedia~=1.4->-r /Users/seankan/Library/Python/3.9/lib/python/site-packages/llama_index/download/llamahub_modules/requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.0.0->wikipedia~=1.4->-r /Users/seankan/Library/Python/3.9/lib/python/site-packages/llama_index/download/llamahub_modules/requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.0.0->wikipedia~=1.4->-r /Users/seankan/Library/Python/3.9/lib/python/site-packages/llama_index/download/llamahub_modules/requirements.txt (line 1)) (2023.11.17)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/seankan/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4->wikipedia~=1.4->-r /Users/seankan/Library/Python/3.9/lib/python/site-packages/llama_index/download/llamahub_modules/requirements.txt (line 1)) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11696 sha256=6cfab3e431b746a478a3c910b16f718c2be103cc8147e59d9e04dd896be1c22b\n",
      "  Stored in directory: /Users/seankan/Library/Caches/pip/wheels/c2/46/f4/caa1bee71096d7b0cdca2f2a2af45cacf35c5760bee8f00948\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "wikipedia_documents = loader.load_data(pages=['University of Georgia', 'Georgia Tech', 'Massachusetts Institute of Technology'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query_engine = VectorStoreIndex.from_documents(\n",
    "    wikipedia_documents, service_context=service_context\n",
    ").as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=vector_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"universities\",\n",
    "            description=\"Wikipedia pages about the universities - University of Georgia, Georgia Tech, Massachusetts Institute of Technology.\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    service_context=service_context,\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[universities] Q: In what states are University of Georgia, Georgia Tech, and Massachusetts Institute of Technology located?\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[universities] A: The University of Georgia is located in the state of Georgia, while Georgia Tech is also located in the state of Georgia. The Massachusetts Institute of Technology (MIT) is located in the state of Massachusetts.\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[universities] Q: What types of programs do University of Georgia, Georgia Tech, and Massachusetts Institute of Technology offer?\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[universities] A: The University of Georgia offers programs at the baccalaureate, master's, and doctoral levels in various fields such as the arts and humanities, business, education, agriculture, and environmental sciences.\n",
      "\n",
      "Georgia Institute of Technology (Georgia Tech) primarily focuses on science, technology, engineering, and mathematics (STEM) fields at both undergraduate and graduate levels, with master's-level courses in Electrical and Computer Engineering, Computer Science, and Mechanical Engineering, as well as Ph.D. coursework in Electrical and Computer Engineering and Mechanical Engineering.\n",
      "\n",
      "Massachusetts Institute of Technology (MIT) offers programs in various disciplines at the undergraduate, graduate, and doctoral levels, including engineering, science, technology, business, and humanities and social sciences. MIT is known for its rigorous academic standards and research-oriented education.\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[universities] Q: Which of these universities have strong research programs?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[universities] A: Both the University of Georgia (UGA) and Massachusetts Institute of Technology (MIT) have strong research programs. The University of Georgia is classified among \"R1: Doctoral Universities – Very high research activity,\" while MIT adopts a European polytechnic university model and stresses laboratory instruction in applied science and engineering, emphasizing an independent faculty engaged in research, as well as instruction oriented around seminars and laboratories. Both universities have produced notable alumni and faculty members in various fields such as Nobel laureates, Turing Award winners, National Medal of Science recipients, and Chief Scientists of the US Air Force.\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[universities] Q: Which of these universities are members of the Association of American Universities?\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[universities] A: Both MIT and UGA have been members of the Association of American Universities since 1900 and 1928, respectively. This information was provided in the context section of the text.\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[universities] Q: How do the admission rates and selectivity of University of Georgia, Georgia Tech, and Massachusetts Institute of Technology compare?\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[universities] A: Based on the provided context, it is not possible to directly compare the admission rates and selectivity of University of Georgia, Georgia Tech, and Massachusetts Institute of Technology (MIT) as this information is only given for Georgia Tech. However, according to the context, U.S. News & World Report categorizes Georgia Tech as \"most selective\" in their 2022 annual ranking of universities. The admission rate for the Class of 2025 at Georgia Tech was 18.3%, with a yield rate of 41.8%. In contrast, the context mentions that MIT is ranked as the second-best national university by U.S. News & World Report in their 2021 rankings and had an overall acceptance rate of 6.7% for the Class of 2025. While Georgia Tech's selectivity is not explicitly compared to University of Georgia, it might be helpful to know that according to the U.S. News & World Report's ranking from last year, the University of Georgia is ranked as a more selective university than both Georgia Tech and MIT in terms of overall acceptance rate for undergraduate admissions. In the fall of 2020, the University of Georgia had an overall acceptance rate of 69.4%. Therefore, it can be said that while Georgia Tech is categorized as \"most selective\" by U.S. News & World Report, both MIT and the University of Georgia are more selective in terms of overall acceptance rates for undergraduate admissions.\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[universities] Q: What notable alumni have graduated from these universities?\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[universities] A: The text provides a list of notable graduates, former students, and current students from Georgia Tech. These include:\n",
      "\n",
      "1. Jimmy Carter, former President of the United States and Nobel Peace Prize winner\n",
      "2. Juan Carlos Varela, President of Panama (class of 1985)\n",
      "3. Kary Mullis, Nobel laureate in Chemistry (class of 1960)\n",
      "4. Charles \"Garry\" Betty, former CEO of Earthlink\n",
      "5. David Dorman, former CEO of AT&T Corporation\n",
      "6. Mike Duke, former CEO of Wal-Mart\n",
      "7. James D. Robinson III, former CEO of American Express and later director of The Coca-Cola Company\n",
      "8. Ivan Allen Jr., former Atlanta mayor and Georgia Tech graduate\n",
      "9. Sam Nunn, former United States Senator\n",
      "10. G. Wayne Clough, former President of Georgia Tech and the first Tech alumnus to serve in that position\n",
      "11. James A. Winnefeld, Jr., former Vice Chairman of the Joint Chiefs of Staff\n",
      "12. Philip M. Breedlove, former Commander, U.S. Air Forces in Europe\n",
      "13. William L. Ball, former Secretary of the Navy\n",
      "14. John M. Brown III, former Commander, United States Army Pacific Command\n",
      "15. Leonard Wood, former Chief of Staff of the Army and Medal of Honor recipient for helping capture Geronimo\n",
      "16. Thomas McGuire, Medal of Honor recipient\n",
      "17. Richard H. Truly, retired Vice Admiral and former administrator of NASA\n",
      "18. John B. \"Jack\" Kerr, former U.S. Senator from Virginia and Georgia Tech trustee\n",
      "19. Robert Woodruff, former chairman and CEO of The Coca-Cola Company\n",
      "20. Jeff Foxworthy, comedian and actor\n",
      "21. Randolph Scott, actor\n",
      "22. Bobby Jones, golf legend and founder of The Masters\n",
      "23. David Duval, former No. 1 ranked golfer in the world\n",
      "24. Chris Bosh, basketball player\n",
      "25. Derrick Favors, basketball player\n",
      "26. Calvin Johnson, football player\n",
      "27. Demaryius Thomas, football player\n",
      "28. Tashard Choice, football player\n",
      "29. Joe Hamilton, former all-time great in football\n",
      "30. Pat Swilling, former all-time great in football\n",
      "31. Billy Shaw, former all-time great in football\n",
      "32. Joe Guyon, former all-time great in football\n",
      "33. Kevin Brown, award-winning baseball player\n",
      "34. Mark Teixeira, baseball player\n",
      "35. Jason Varitek, baseball player\n",
      "\n",
      "Note: The list may not be exhaustive as the text mentions that there are many more notable alumni from Georgia Tech in various fields such as engineering, business, and athletics.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"Give me all similaries between University of Georgia, Georgia Tech, Massachusetts Institute of Technology\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** 1. Located in the United States: All three universities are physically located within the boundaries of the United States.\n",
       "2. Research Universities: Both University of Georgia and Massachusetts Institute of Technology (MIT) have strong research programs, while Georgia Tech primarily focuses on science, technology, engineering, and mathematics (STEM) fields at both undergraduate and graduate levels, with master's-level courses in Electrical and Computer Engineering, Computer Science, and Mechanical Engineering, as well as Ph.D. coursework in Electrical and Computer Engineering and Mechanical Engineering.\n",
       "3. Members of Association of American Universities: Both MIT and UGA have been members of the Association of American Universities since 1900 and 1928, respectively. This indicates that all three universities are recognized as leading institutions in their respective states and across the country in terms of academic excellence and research output.\n",
       "4. Notable Alumni: All three universities have produced notable alumni in various fields such as politics, business, sports, entertainment, academia, and military service, some of whom have received prestigious awards like Nobel Prizes, Turing Awards, National Medals of Science, etc.\n",
       "5. Rankings: While the rankings may vary from year to year, all three universities have consistently been ranked among the top research universities in their respective states and nationally by various publications such as U.S. News & World Report, Times Higher Education, QS World University Rankings, and Academic Ranking of World Universities (ARWU).\n",
       "6. State Affiliations: All three universities are affiliated with their respective states in which they are located - the University of Georgia is located in the state of Georgia, while Georgia Tech and MIT are both located in the state of Massachusetts. This may indicate that all three universities have a strong influence on the academic and research communities within their respective states, as well as contribute to the economic and cultural development of those states through various initiatives, partnerships, and collaborations with industry, government, and other academic institutions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Demo of loading PDFs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seankan/Library/Python/3.9/lib/python/site-packages/pypdf/generic/_base.py:391: RuntimeWarning: coroutine 'run_async_tasks.<locals>._gather' was never awaited\n",
      "  return float.__new__(cls, value)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/seankan/Library/Python/3.9/lib/python/site-packages/pypdf/generic/_base.py:391: RuntimeWarning: coroutine 'VectorStoreIndex._async_add_nodes_to_index' was never awaited\n",
      "  return float.__new__(cls, value)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/seankan/Library/Python/3.9/lib/python/site-packages/pypdf/generic/_base.py:391: RuntimeWarning: coroutine 'SubQuestionQueryEngine._aquery_subq' was never awaited\n",
      "  return float.__new__(cls, value)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    \"data/annual_reports/\"\n",
    ")\n",
    "\n",
    "pdf_documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query_engine = VectorStoreIndex.from_documents(\n",
    "    pdf_documents, service_context=service_context\n",
    ").as_query_engine()\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=vector_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"annual reports\",\n",
    "            description=\"Annual reports of three different property developers in Hong Kong at the year of 2022.\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    service_context=service_context,\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[annual reports] Q: What is the profitability of Company A as mentioned in the annual reports?\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[annual reports] A: To determine the profitability of Company A based on the provided context information, we need to analyze the financial statements presented in the annual reports.\n",
      "\n",
      "Firstly, we can see that Company A's profit before taxation for the year 201x is stated as 9,214.5 in page 235 of the first report and 10,332.5 in page 236 of the second report. After deducting the applicable taxes, Company A's profit for the year in 201x is 4,301.8 (page 235) and 4,670.9 (page 236).\n",
      "\n",
      "Comparing these figures across both years, we can see that Company A had higher profits in the second year. Specifically, in 201x, Company A's profit increased by approximately 670.1 (page 236 - page 235) or 15.4% (page 236 profit / page 235 profit).\n",
      "\n",
      "Therefore, based on the financial information provided in these annual reports, Company A's profitability improved from the previous year.\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[annual reports] Q: What is the profitability of Company B as mentioned in the annual reports?\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[annual reports] A: The provided context information includes income statements for two different years and companies. The company named \"CK Hutchison Holdings Limited\" is mentioned in the second context, but it does not provide sufficient details to determine its profitability for a specific year. However, in the first context, the company's profit after tax for ordinary shareholders in 2021 and 2022 is given as 33,484 HK$ and 36,680 HK$, respectively. To convert these figures to a different currency or determine their equivalence in US dollars (mentioned in the first context), the exchange rate for the respective years would be required. Without this information, it is not possible to compare the profitability of Company B with that of CK Hutchison Holdings Limited.\n",
      "\n",
      "Regarding Company B, there is no information provided in either annual report about its profitability in 2017 or 2018 (implied from the segment assets and liabilities listed). Therefore, it is not possible to determine its profitability for those years based on this context alone.\n",
      "\n",
      "Overall, without additional context or details about Company B's financial performance, it cannot be compared with any other company mentioned in these annual reports.\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[annual reports] Q: What is the profitability of Company C as mentioned in the annual reports?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[annual reports] A: The provided context information includes income statements for two different companies, Company A and Company C. From the income statement for Company C (CK Hutchison Holdings Limited) in 2022, we can see that their profit after tax was HK$43,484 million or US$5.61 per share (page 236). This information is presented in the annual report data provided under \"Data > Annual Reports\" for Company C's 2022 report (ar2022.pdf). In contrast, we do not have any profitability information for a company referred to as Company C in the income statement presented for Company A in their 2022 annual report (data/annual_reports/AR2022.pdf). Therefore, it is unclear if there is another Company C that was not mentioned in the provided context information or if there is a mistake in labeling Company C's data in the income statement presented for Company A's 2022 report. Without further contextual information, it is best to assume that the profitability figures provided are for Company C (CK Hutchison Holdings Limited) and not another company with a similar name.\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[annual reports] Q: Which company among Company A, Company B and Company C has the highest return in year 2022 according to the annual reports? \n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[annual reports] A: There is no information provided in the given context that allows for a comparison of returns among Company A, Company B, and Company C in year 2022. The query should be amended with more context or additional information to allow for such a comparison.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"Compare the three different companies in terms of profitability, which one has the highest return in year 2022\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Based on the provided context information, it is not possible to compare the profitability and determine which company among Company A, Company B, and Company C has the highest return in year 2022. While we have profitability figures for Companies A and C (CK Hutchison Holdings Limited) from their respective annual reports, there is no information provided about Company B's profitability in 2022. Additionally, it is unclear if there is a mistake in labeling Company C's data in the income statement presented for Company A's 2022 report or if there is another Company C that was not mentioned in the provided context information. Without further contextual information, it is best to assume that the profitability figures provided are for Company C (CK Hutchison Holdings Limited) and not another company with a similar name. Therefore, to compare the three different companies in terms of profitability and determine which one has the highest return in year 2022, more context or additional information is required."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[annual reports] Q: Which property developer's annual report should I refer to in order to identify the leaders of Sun Hung Kai?\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[annual reports] A: The query asks which property developer's annual report should be referred to in order to identify the leaders of Sun Hung Kai. Based on the context information provided, it is clear that the annual report being referred to is for Sun Hung Kai Properties Limited (SHKPAR). Therefore, the relevant section from their annual report should be consulted to learn about the company's Board of Directors and Committees, which will provide information on the leaders of Sun Hung Kai.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"Who are the leaders of Sun Hung Kai, New World Development Company and CK Hutchison?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** To identify the leaders of Sun Hung Kai, based on the context information provided, we need to refer to their annual report. The query does not specify which year's annual report should be consulted, so we can assume it is for the most recent financial year. As per the latest annual report of Sun Hung Kai Properties Limited (SHKPAR), as of 2021, the Board of Directors consists of:\n",
       "\n",
       "- Ronnie C. Chan - Chairman\n",
       "- Raymond Kwok Ki-sun - Managing Director\n",
       "- Cindy S. K. Chan - Executive Director\n",
       "- Alexis Lau Ka-fai - Independent Non-executive Director\n",
       "- Paul Y.S. Chu - Independent Non-executive Director\n",
       "- Michael W.T. Fung - Independent Non-executive Director\n",
       "- Rebecca R. Y. Cheung - Independent Non-executive Director\n",
       "- Joseph K. Ng Ka-ki - Senior Vice President and Chief Financial Officer\n",
       "- Tommy Lai Ming-kam - Executive Vice President\n",
       "- Terence Chan Tze-leung - Executive Vice President\n",
       "\n",
       "Regarding New World Development Company, we can also refer to their latest annual report, as per the context information provided. As of 2021, according to their annual report, the Board of Directors consists of:\n",
       "\n",
       "- Cheng Kar-shun (KC) - Chairman\n",
       "- Adrian Cheng Chi-kai - Managing Director and Executive Director\n",
       "- Kenneth Wong Kin-keung - Deputy Managing Director and Executive Director\n",
       "- John C. Chau Chor-kiu - Independent Non-executive Director\n",
       "- Joseph Fan Yu-ming - Senior Independent Non-executive Director\n",
       "- Florence Po Chiu-lan - Independent Non-executive Director\n",
       "- Peter Lee Ka-kit - Independent Non-executive Director\n",
       "- Henry Lau Yip-shing - Deputy Chairman and Executive Director (Property) of NWD's subsidiary New World China Land Limited\n",
       "\n",
       "For CK Hutchison, the context information provided does not specify if it is referring to their overall group or a specific company within the conglomerate. If we assume that the query refers to the parent company CK Hutchison Holdings Limited (CKHH), according to their latest annual report as of 2021, the Board of Directors consists of:\n",
       "\n",
       "- Victor Li Tzar-kuoi - Chairman and Managing Director\n",
       "- Canning Fok Kin-ning - Deputy Managing Director and Executive Director\n",
       "- Allan Zeman Kung - Independent Non-executive Director\n",
       "- Henry Lau Yip-shing - Senior Independent Non-executive Director\n",
       "- Dato' Sri Lau Ban Seng - Independent Non-executive Director\n",
       "- Florence Po Chiu-lan - Independent Non-executive Director\n",
       "- Joseph Fan Yu-ming - Senior Independent Non-executive Director\n",
       "- Peter Lee Ka-kit - Independent Non-executive Director\n",
       "\n",
       "I hope this helps!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Demo of Different Documents inside the same folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The Geely Automobile Group introduced their first pure electric Galaxy model in China with the aim of attracting budget-sensitive mainstream buyers from competitors like BYD and foreign brands. The E8 sedan, priced at 175,800 yuan (US$24,752), has a driving range of 550 kilometers and is positioned as an ideal model to replace both existing petrol and electric cars due to its superiority in safety, design, performance, and intelligence compared to rival blockbuster models like BYD's Hanelectric vehicle. The E8 also features a 45-inch screen supplied by display panel manufacturer BOE Technology. The launch of the E8 is part of Geely's efforts to intensify competition in China's EV market, where carmakers have missed their sales goal for 2023 due to increasing competition. (Source: \"Chinese EV maker Geely introduces first pure electric Galaxy model, to woo mainstream buyers from BYD, foreign brands | South China Morning Post,\" scmp.com, January 8, 2024)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data/Mixed/\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context,service_context=service_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine(response_mode = \"refine\")\n",
    "response = query_engine.query(\"What did the Geely Automobile Group do in China? provide the response along the source file and its filepath\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The documents included within an index during the presentation of financial results may vary from year to year, but typically include a range of financial and governance-related information. Some examples of documents that might be found within an annual report index are:\n",
       "\n",
       "1. Corporate Information - This section provides an overview of the company's structure, history, and business activities. It can contain subsections such as Contents, Corporate Profile, Analyses of Core Business Segments by Geographical Location, Analyses by Core Business Segments, Key Financial Information, Business Highlights, Chairman’s Statement, Operations Review, Group Capital Resources and Liquidity, Risk Factors, Information on Directors, Information on Senior Management, Directors’ Report, Corporate Governance Report, Independent Auditor’s Report, Consolidated Income Statement, Consolidated Statement of Comprehensive Income, Consolidated Statement of Financial Position, Consolidated Statement of Changes in Equity, Consolidated Statement of Cash Flows, Notes to the Financial Statements, Principal Subsidiary and Associated Companies and Joint Ventures, Ten Year Summary, and Information for Shareholders.\n",
       "\n",
       "2. Bond information - This section provides details on the company's bond offerings, including issuance dates, amounts, interest rates, maturity dates, and other relevant financial data.\n",
       "\n",
       "3. Currency-specific information - For companies with international operations or borrowing, this section might include details about bonds issued in specific currencies, such as EUR750 million notes, 3.625% due 2022 for the euro or GBP303 million notes, 5.625% due 2026 for the British pound sterling.\n",
       "\n",
       "4. Risk factors - This section explains the major risks and uncertainties associated with the company's operations and financial position, including market risks, credit risks, operational risks, compliance risks, strategic risks, legal risks, reputational risks, and political risks.\n",
       "\n",
       "5. Directors’ report - This document provides a detailed overview of the company's performance during the year, major events and transactions, and strategies for the future. It can also include information about key personnel, remuneration policies, and governance practices.\n",
       "\n",
       "6. Independent auditor's report - This document is an independent assessment of the company's financial statements prepared in accordance with specific accounting standards or laws. It confirms the accuracy and reliability of the financial data presented in the annual report.\n",
       "\n",
       "7. Shareholder information - This section provides details about shareholders' rights, dividends, voting procedures, and other relevant information for investors.\n",
       "\n",
       "Overall, an annual report index is a comprehensive collection of documents that aims to provide stakeholders with all the necessary financial, operational, and governance-related information in one place."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"What are the documents inside the index?\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The Geely Automobile Group recently partnered with Shanghai-based Nio to promote battery-swapping technology in China as both companies work to address the issue of insufficient charging infrastructure. This information can be found in the source document \"Chinese EV maker Geely introduces first pure electric Galaxy model, to woo mainstream buyers from BYD, foreign brands | South China Morning Post.\" With the formation of this partnership in November, battery-swapping technology will allow owners of electric cars to quickly exchange a spent battery for a fully charged one. This initiative is particularly significant given that sales of battery-powered vehicles in mainland China are projected to grow by 20% annually through 2024, as reported by Fitch Ratings in November."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"What did the Geely Automobile Group do in China, let me know the answer and also the name of the source document.\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Based on the provided context, it seems that this query engine is related to searching through the Annual Report 2021/22 of Sun Hung Kai Properties Limited (SHKPAR) for specific information or documents. The context includes page numbers and file paths for certain sections within the report, as well as the title and publication year of the report itself. Therefore, it's reasonable to assume that this query engine allows users to search for keywords, phrases, or other criteria within the text of the annual report, potentially including financial statements, executive summaries, and other important documents related to SHKPAR's operations during the 2021/22 fiscal year. Without further information about the specific capabilities or limitations of this query engine, it's impossible to provide a more detailed answer, but at least we now have some context that can help guide our understanding and interpretation of its functionality."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"What documents are included in this query engine?\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Demo of Mixtral 8*7B to the same query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Based on the provided context, Geely Automobile Group, a Chinese automaker, introduced its first pure electric Galaxy model to attract mainstream buyers away from competitors like BYD and foreign brands. The company has also formed a partnership with Shanghai-based Nio, another electric vehicle (EV) manufacturer, to promote battery-swapping technology. This collaboration aims to address the issue of inadequate charging infrastructure in China, as mentioned in a November 2024 article from the South China Morning Post titled \"Chinese EV maker Geely introduces first pure electric Galaxy model, to woo mainstream buyers from BYD, foreign brands\" (data/Mixed/Chinese EV maker Geely introduces first pure electric Galaxy model, to woo mainstream buyers from BYD, foreign brands | South China Morning Post.pdf). According to a Fitch Ratings report and the China Passenger Car Association, sales of battery-powered vehicles in mainland China are growing by 20% year on year in 2024, but only a few manufacturers, including BYD and Li Auto, are profitable. A new round of price cuts is also in effect, with top players like BYD and Xpeng offering discounts to attract buyers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mixtral 8*7B model\n",
    "llm = Ollama(\n",
    "  model=\"mixtral\",\n",
    "  context_window=3900,\n",
    "  messages_to_prompt=messages_to_prompt,\n",
    "  temperature = 0.7,\n",
    "  additional_kwargs={\"top_k\": 50, \"top_p\": 0.95}\n",
    "  )\n",
    "\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local\")\n",
    "\n",
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data/Mixed/\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context,service_context=service_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine(response_mode = \"refine\")\n",
    "response = query_engine.query(\"What did the Geely Automobile Group do in China, let me know the answer and also the name of the source document.\")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Parsing**\n",
    "### **Creating LlamaIndex Nodes**\n",
    "In LlamaIndex, once the data has been ingested and represented as Documents, there's an option to further process these Documents into Nodes. Nodes are more granular data entities that represent \"chunks\" of source Documents, which could be text chunks, images, or other types of data. They also carry metadata and relationship information with other nodes, which can be instrumental in building a more structured and relational index.\n",
    "\n",
    "<img src=\"/Users/seankan/Desktop/llama_index_test/img/nodes.png\" alt=\"alt text\" width=\"450\" />\n",
    "\n",
    "#### **Basic**\n",
    "\n",
    "To parse Documents into Nodes, LlamaIndex provides NodeParser classes. These classes help in automatically transforming the content of Documents into Nodes, adhering to a specific structure that can be utilized further in index construction and querying.\n",
    "\n",
    "Here's how you can use a SimpleNodeParser to parse your Documents into Nodes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author, Paul Graham, grew up working on two main things outside of school: writing short stories and programming on an IBM 1401 in the basement of their junior high school using an early version of Fortran. They also waited for years to convince their father to buy a microcomputer, a TRS-80, where they could write simple games, programs to predict how high their model rockets would fly, and a word processor for their father's use. These activities were pursued before the author decided to study philosophy in college, but eventually switched to AI due to being drawn into the world of a novel featuring an intelligent computer called Mike and witnessing Terry Winograd using SHRDLU on PBS.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import SimpleDirectoryReader\n",
    "import tiktoken\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data/paul_graham/\").load_data()\n",
    "\n",
    "# Initialize the parser\n",
    "node_parser = SimpleNodeParser.from_defaults(\n",
    "    separator=\" \",\n",
    "    chunk_size=1024, \n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "# Parse documents into nodes\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# Assuming nodes is your list of Node objects\n",
    "index = VectorStoreIndex(nodes, service_context=service_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author, Paul Graham, is discussing his experience with computers and programming languages. He mentions that his experience skipped a step in the evolution of computers, as he went from batch processing to microcomputers without experiencing time-sharing machines with interactive OSes. The author also explains that he recently completed working on a new programming language called Bel, which was written in itself using an egregious collection of hacks. He had to put aside writing essays for several years to focus on completing Bel, as it required a lot of intense work and problem-solving. Graham also discusses how he chooses what projects to work on, as he reflects on his past choices. Overall, the context can be summarized as the author's experiences with computers, programming languages, and his recent completion of working on a new language called Bel.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Summarise the context\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fine Tuning**\n",
    "\n",
    "Finetuning a model means updating the model itself over a set of data to improve the model in a variety of ways. This can include improving the quality of outputs, reducing hallucinations, memorizing more data holistically, and reducing latency/cost.\n",
    "\n",
    "The core of our toolkit revolves around in-context learning / retrieval augmentation, which involves using the models in inference mode and not training the models themselves.\n",
    "\n",
    "While finetuning can be also used to “augment” a model with external data, finetuning can complement retrieval augmentation in a variety of ways:\n",
    "\n",
    "## **Embedding Finetuning Benefits**\n",
    "Finetuning the embedding model can allow for more meaningful embedding representations over a training distribution of data –> leads to better retrieval performance.\n",
    "\n",
    "## **LLM Finetuning Benefits**\n",
    "Allow it to learn a style over a given dataset\n",
    "\n",
    "Allow it to learn a DSL that might be less represented in the training data (e.g. SQL)\n",
    "\n",
    "Allow it to correct hallucinations/errors that might be hard to fix through prompt engineering\n",
    "\n",
    "Allow it to distill a better model (e.g. GPT-4) into a simpler/cheaper model (e.g. gpt-3.5, Llama 2)\n",
    "\n",
    "### **Integrations with LlamaIndex**\n",
    "This is an evolving guide, and there are currently three key integrations with LlamaIndex. Please check out the sections below for more details!\n",
    "\n",
    "Finetuning embeddings for better retrieval performance\n",
    "\n",
    "Finetuning Llama 2 for better text-to-SQL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
